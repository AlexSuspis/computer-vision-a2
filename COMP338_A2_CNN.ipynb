{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP338-A2-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N-AcTUNtzCIq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP338 Assignment 2 - Convolutional Neural Networks (CNNs)"
      ],
      "metadata": {
        "id": "IVxBgoZFdVFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREREQUISITES: Importing The Dataset From GDrive ###"
      ],
      "metadata": {
        "id": "N-AcTUNtzCIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import time\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr-zFvdZzFIa",
        "outputId": "4cd968f9-163b-426a-a91d-acb5a2278075"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA LOADER #\n",
        "\n",
        "'''\n",
        "Assignment 2\n",
        "\n",
        "author  : Shan Luo\n",
        "created : 20/11/20 5:30 PM\n",
        "'''\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import scipy.io as scio\n",
        "from skimage import io\n",
        "import pdb\n",
        "\n",
        "\n",
        "\n",
        "class imageDataset(Dataset): \n",
        "\n",
        "    def __init__(self, root_dir, file_path, imSize = 250, shuffle=False):\n",
        "        self.imPath = np.load(file_path) \n",
        "        self.root_dir = root_dir\n",
        "        self.imSize = imSize\n",
        "        self.file_path=file_path\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imPath)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "    \t# print(self.root_dir)\n",
        "    \t# print(self.imPath[idx])\n",
        "        im = io.imread(os.path.join(self.root_dir, self.imPath[idx]))  # read the image\n",
        "\n",
        "        if len(im.shape) < 3: # if there is grey scale image, expand to r,g,b 3 channels\n",
        "            im = np.expand_dims(im, axis=-1)\n",
        "            im = np.repeat(im,3,axis = 2)\n",
        "\n",
        "        img_folder = self.imPath[idx].split('/')[-2]\n",
        "        if img_folder =='faces':\n",
        "            label = np.zeros((1, 1), dtype=int)\n",
        "        elif img_folder == 'dog':\n",
        "            label = np.zeros((1, 1), dtype=int)+1\n",
        "        elif img_folder == 'airplanes':\n",
        "            label = np.zeros((1, 1), dtype=int)+2\n",
        "        elif img_folder == 'keyboard':\n",
        "            label = np.zeros((1, 1), dtype=int)+3\n",
        "        elif img_folder == 'cars':\n",
        "            label = np.zeros((1, 1), dtype=int)+4\n",
        "\n",
        "\n",
        "        img = np.zeros([3,im.shape[0],im.shape[1]]) # reshape the image from HxWx3 to 3xHxW\n",
        "        img[0,:,:] = im[:,:,0]\n",
        "        img[1,:,:] = im[:,:,1]\n",
        "        img[2,:,:] = im[:,:,2]\n",
        "\n",
        "        imNorm = np.zeros([3,im.shape[0],im.shape[1]]) # normalize the image\n",
        "        imNorm[0, :, :] = (img[0,:,:] - np.max(img[0,:,:]))/(np.max(img[0,:,:])-np.min(img[0,:,:])) -0.5\n",
        "        imNorm[1, :, :] = (img[1,:,:] - np.max(img[1,:,:]))/(np.max(img[1,:,:])-np.min(img[1,:,:])) -0.5\n",
        "        imNorm[2, :, :] = (img[2,:,:] - np.max(img[2,:,:]))/(np.max(img[2,:,:])-np.min(img[2,:,:])) -0.5\n",
        "\n",
        "        return{\n",
        "            'imNorm': imNorm.astype(np.float32),\n",
        "            'label':np.transpose(label.astype(np.float32))                  #image label\n",
        "            }\n",
        "\n",
        "class DefaultTrainSet(imageDataset):\n",
        "    def __init__(self, **kwargs):\n",
        "        script_folder = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) \n",
        "         #  img_list_train.npy that contains the path of the training images is provided \n",
        "        default_path = os.path.join(script_folder, 'img_list_train.npy')\n",
        "        root_dir = os.path.join(script_folder, 'data')\n",
        "        super(DefaultTrainSet, self).__init__(root_dir, file_path=default_path, imSize = 250,**kwargs)\n",
        "\n",
        "\n",
        "class DefaultTestSet(imageDataset):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        script_folder = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "        #  img_list_test.npy that contains the path of the testing images is provided\n",
        "        default_path = os.path.join(script_folder, 'img_list_test.npy')  \n",
        "        root_dir = os.path.join(script_folder, 'data')\n",
        "        super(DefaultTestSet, self).__init__(root_dir, file_path=default_path, imSize = 250,**kwargs)\n",
        "  "
      ],
      "metadata": {
        "id": "JYMY_WVRzYEB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Creating datasets fro the training and test images\n",
        "# using FashionMNIST\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"drive/My Drive/COMP338_Assignment2_Dataset/training\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"drive/My Drive/COMP338_Assignment2_Dataset/test\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "3IYOZpA7zhz8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A test to see whether the images were imported successfully\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "0zNHPY6v1IPm",
        "outputId": "6d679848-2b49-4bbe-9280-6798f1f96501"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([16, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([16])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 (40 marks) Feature extraction using a Convolution Neural Network ##"
      ],
      "metadata": {
        "id": "ffi8VT2udcng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Install PyTorch and Torchvision ###"
      ],
      "metadata": {
        "id": "Hnt5hgsQeekq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0y5PLM6ciB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0accea-5124-4f67-fe19-942679f5784f"
      },
      "source": [
        "!pip install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install torchvision"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: torch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "LcGVaagRcav8"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import torch.optim as optim"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.cnn_layers = Sequential(\n",
        "            # First Hidden Layer\n",
        "            Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            BatchNorm2d(64),\n",
        "            ReLU(inplace=True),\n",
        "\n",
        "            # Second Hidden Layer (Pooling Layer 1)\n",
        "            MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
        "\n",
        "            # Third Hidden Layer\n",
        "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(64),\n",
        "            ReLU(inplace=True),\n",
        "\n",
        "            # Fourth Hidden Layer (Pooling Layer 2)\n",
        "            MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            # Fully Connected Layer\n",
        "            Linear(3 * 250 * 250, 64 * 5)\n",
        "        )\n",
        "\n",
        "        \n",
        "\n",
        "        '''\n",
        "        # First Hidden Layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "\n",
        "        # Second Hidden Layer (Pooling Layer 1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "        # Third Hidden Layer\n",
        "        self.conv2 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Fourth Hidden Layer (Pooling Layer 2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
        "        \n",
        "        # cf comments in forward() to have step by step comments\n",
        "        # on the shape (how we pass from a 3x32x32 input image to a 18x16x16 volume)\n",
        "        self.fc = nn.Linear(18 * 16 * 16, 64)\n",
        "\n",
        "        '''\n",
        "        #self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "     # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "l6vPmbY4fVh3"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN2, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        # cf comments in forward() to have step by step comments\n",
        "        # on the shape (how we pass from a 3x32x32 input image to a 18x16x16 volume)\n",
        "        self.fc1 = nn.Linear(18 * 16 * 16, 64) \n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass,\n",
        "        x shape is (batch_size, 3, 32, 32)\n",
        "        (color channel first)\n",
        "        in the comments, we omit the batch_size in the shape\n",
        "        \"\"\"\n",
        "        # shape : 3x32x32 -> 18x32x32 (Recall: (N -F+2P)/S+1) = (32-3+2*1)/1+1=32\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # 18x32x32 -> 18x16x16 (Recall: (N -F+2P)/S+1) = (32-2+2*0)/2+1 = 16\n",
        "        x = self.pool(x)\n",
        "        # 18x16x16 -> 4608\n",
        "        x = x.view(-1, 18 * 16 * 16)\n",
        "        # 4608 -> 64\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # 64 -> 10\n",
        "        # The softmax non-linearity is applied later (cf createLossAndOptimizer() fn)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gbVbE5yo7TK4"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmaxProbabiities(e):\n",
        "  sm = nn.Softmax2d()\n",
        "  e = sm(e)\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "H3XHRCFtvdsN"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - (10 marks) Define a loss function and the optimizer ##"
      ],
      "metadata": {
        "id": "gNRvDYx3txQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    # it combines softmax with negative log likelihood loss\n",
        "    criterion = nn.CrossEntropyLoss()  \n",
        "    #optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    return criterion, optimizer"
      ],
      "metadata": {
        "id": "HsKNpbqxt024"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 - (20 marks) Train The Network ##"
      ],
      "metadata": {
        "id": "ZYzcIXBov7Eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "n_training_samples = 20000 # Max: 50 000 - n_val_samples\n",
        "n_val_samples = 5000\n",
        "n_test_samples = 5000\n",
        "\n",
        "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
        "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
        "# (In the last case, indexes do not need to account for training ones because the train=False parameter in datasets.CIFAR will select from the test set)"
      ],
      "metadata": {
        "id": "0reiqPUB3BQH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Data Loader #\n",
        "\n",
        "def get_train_loader(batch_size):\n",
        "    return torch.utils.data.DataLoader(training_data, batch_size=batch_size, sampler=train_sampler,\n",
        "                                              num_workers=2)\n",
        "\n",
        "# Use larger batch size for validation to speed up computation\n",
        "val_loader = torch.utils.data.DataLoader(training_data, batch_size=64, sampler=val_sampler,\n",
        "                                          num_workers=2)"
      ],
      "metadata": {
        "id": "8tzi11Yv2ILg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "if th.cuda.is_available():\n",
        "  # Make CuDNN Determinist\n",
        "  th.backends.cudnn.deterministic = True\n",
        "  th.cuda.manual_seed(seed)\n",
        "\n",
        "# Define default device, we should use the GPU (cuda) if available\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nYY12XzP31Xj"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, batch_size, n_epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Train a neural network and print statistics of the training\n",
        "    \n",
        "    :param net: (PyTorch Neural Network)\n",
        "    :param batch_size: (int)\n",
        "    :param n_epochs: (int)  Number of iterations on the training set\n",
        "    :param learning_rate: (float) learning rate used by the optimizer\n",
        "    \"\"\"\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"n_epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_minibatches = len(train_loader)\n",
        "\n",
        "    criterion, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "    # Init variables used for plotting the loss\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "    \n",
        "    training_start_time = time.time()\n",
        "    best_error = np.inf\n",
        "    best_model_path = \"best_model.pth\"\n",
        "    \n",
        "    # Move model to gpu if possible\n",
        "    net = net.to(device)\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        print_every = n_minibatches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "            # Move tensors to correct device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            print(inputs)\n",
        "            outputs = net(inputs)\n",
        "            print(\"Hello\")\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "           \n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "\n",
        "            # print every 10th of epoch\n",
        "            if (i + 1) % (print_every + 1) == 0:    \n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                      epoch + 1, int(100 * (i + 1) / n_minibatches), running_loss / print_every,\n",
        "                      time.time() - start_time))\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "\n",
        "        train_history.append(total_train_loss / len(train_loader))\n",
        "\n",
        "        total_val_loss = 0\n",
        "        # Do a pass on the validation set\n",
        "        # We don't need to compute gradient,\n",
        "        # we save memory and computation using th.no_grad()\n",
        "        with th.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              # Move tensors to correct device\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              # Forward pass\n",
        "              predictions = net(inputs)\n",
        "              val_loss = criterion(predictions, labels)\n",
        "              total_val_loss += val_loss.item()\n",
        "            \n",
        "        val_history.append(total_val_loss / len(val_loader))\n",
        "        # Save model that performs best on validation set\n",
        "        if total_val_loss < best_error:\n",
        "            best_error = total_val_loss\n",
        "            th.save(net.state_dict(), best_model_path)\n",
        "\n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
        "\n",
        "    print(\"Training Finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
        "    \n",
        "    # Load best model\n",
        "    net.load_state_dict(th.load(best_model_path))\n",
        "    \n",
        "    return train_history, val_history"
      ],
      "metadata": {
        "id": "MJNfUCzb13yB"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = CNN()\n",
        "\n",
        "x = torch.randn(2, 3, 37, 37)\n",
        "\n",
        "\n",
        "#train_history, val_history = train(net, batch_size=16, n_epochs=20, learning_rate=0.01)"
      ],
      "metadata": {
        "id": "mMGGgRLu1499"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECOND VERSION\n",
        "\n",
        "# defining the number of epochs\n",
        "n_epochs = 25\n",
        "# empty list to store training losses\n",
        "train_losses = []\n",
        "# empty list to store validation losses\n",
        "val_losses = []\n",
        "\n",
        "def train(epoch):\n",
        "    net.train()\n",
        "    tr_loss = 0\n",
        "    # getting the training set\n",
        "    x_train, y_train = Variable(training_data), Variable(training_data.train_labels)\n",
        "    # getting the validation set\n",
        "    x_val, y_val = Variable(test_data), Variable(test_data.test_labels)\n",
        "    # converting the data into GPU format\n",
        "    if torch.cuda.is_available():\n",
        "        x_train = x_train.cuda()\n",
        "        y_train = y_train.cuda()\n",
        "        x_val = x_val.cuda()\n",
        "        y_val = y_val.cuda()\n",
        "\n",
        "    criterion, optimizer = createLossAndOptimizer(net, 0.001)\n",
        "\n",
        "    # clearing the Gradients of the model parameters\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # prediction for training and validation set\n",
        "    output_train = net(x_train)\n",
        "    output_val = net(x_val)\n",
        "\n",
        "    # computing the training and validation loss\n",
        "    loss_train = criterion(output_train, y_train)\n",
        "    loss_val = criterion(output_val, y_val)\n",
        "    train_losses.append(loss_train)\n",
        "    val_losses.append(loss_val)\n",
        "\n",
        "    # computing the updated weights of all the model parameters\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    tr_loss = loss_train.item()\n",
        "    if epoch%2 == 0:\n",
        "        # printing the validation loss\n",
        "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "4v-XpvE18r3n",
        "outputId": "fbf46b8c-d81f-4ebe-a2a7-3c93e14b468e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-826a93cc42d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-90-826a93cc42d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# getting the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# getting the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Variable data has to be a tensor, but got FashionMNIST"
          ]
        }
      ]
    }
  ]
}